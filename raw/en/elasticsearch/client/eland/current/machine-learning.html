<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Machine Learning | Eland Python Client | Elastic</title>
<link rel="home" href="index.html" title="Eland Python Client"/>
<link rel="up" href="index.html" title="Eland Python Client"/>
<link rel="prev" href="dataframes.html" title="Data Frames"/>
<meta name="DC.type" content="Learn/Docs/Clients/eland"/>
<meta name="DC.subject" content="Clients"/>
<meta name="DC.identifier" content="master"/>
</head>
<body>
<div id="content">
<div class="breadcrumbs">
<span class="breadcrumb-link"><a href="index.html">Eland Python Client</a></span>
»
<span class="breadcrumb-node">Machine Learning</span>
</div>
<div class="navheader">
<span class="prev">
<a href="dataframes.html">« Data Frames</a>
</span>
<span class="next">
</span>
</div>
<div class="chapter">
<div class="titlepage"><div><div>
<h1 class="title"><a id="machine-learning"></a>Machine Learning<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/eland/edit/main/docs/guide/machine-learning.asciidoc">edit</a></h1>
</div></div></div>
<h3><a id="ml-trained-models"></a>Trained models<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/eland/edit/main/docs/guide/machine-learning.asciidoc">edit</a></h3>
<p>Eland allows transforming trained models from scikit-learn, XGBoost,
and LightGBM libraries to be serialized and used as an inference
model in Elasticsearch.</p>
<div class="pre_wrapper lang-python">
<pre class="programlisting prettyprint lang-python">&gt;&gt;&gt; from xgboost import XGBClassifier
&gt;&gt;&gt; from eland.ml import MLModel

# Train and exercise an XGBoost ML model locally
&gt;&gt;&gt; xgb_model = XGBClassifier(booster="gbtree")
&gt;&gt;&gt; xgb_model.fit(training_data[0], training_data[1])

&gt;&gt;&gt; xgb_model.predict(training_data[0])
[0 1 1 0 1 0 0 0 1 0]

# Import the model into Elasticsearch
&gt;&gt;&gt; es_model = MLModel.import_model(
    es_client="http://localhost:9200",
    model_id="xgb-classifier",
    model=xgb_model,
    feature_names=["f0", "f1", "f2", "f3", "f4"],
)

# Exercise the ML model in Elasticsearch with the training data
&gt;&gt;&gt; es_model.predict(training_data[0])
[0 1 1 0 1 0 0 0 1 0]</pre>
</div>
<h3><a id="ml-nlp-pytorch"></a>Natural language processing (NLP) with PyTorch<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/eland/edit/main/docs/guide/machine-learning.asciidoc">edit</a></h3>
<div class="important admon">
<div class="icon"></div>
<div class="admon_content">
<p>You need to use PyTorch <code class="literal">1.11.0</code> or earlier to import an NLP model.
Run <code class="literal">pip install torch==1.11</code> to install the aproppriate version of PyTorch.</p>
</div>
</div>
<p>For NLP tasks, Eland enables you to import PyTorch trained BERT models into Elasticsearch.
Models can be either plain PyTorch models, or supported
<a href="https://huggingface.co/transformers" class="ulink" target="_top">transformers</a> models from the
<a href="https://huggingface.co/models" class="ulink" target="_top">Hugging Face model hub</a>. For example:</p>
<div class="pre_wrapper lang-bash">
<pre class="programlisting prettyprint lang-bash">$ eland_import_hub_model &lt;authentication&gt; \ <a id="CO1-1"></a><i class="conum" data-value="1"></i>
  --url http://localhost:9200/ \ <a id="CO1-2"></a><i class="conum" data-value="2"></i>
  --hub-model-id elastic/distilbert-base-cased-finetuned-conll03-english \ <a id="CO1-3"></a><i class="conum" data-value="3"></i>
  --task-type ner \ <a id="CO1-4"></a><i class="conum" data-value="4"></i>
  --start</pre>
</div>
<div class="calloutlist">
<table border="0" summary="Callout list">
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO1-1"><i class="conum" data-value="1"></i></a></p>
</td>
<td align="left" valign="top">
<p>Use an authentication method to access your cluster. Refer to <a class="xref" href="machine-learning.html#ml-nlp-pytorch-auth" title="Authentication methods">Authentication methods</a>.</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO1-2"><i class="conum" data-value="2"></i></a></p>
</td>
<td align="left" valign="top">
<p>The cluster URL. Alternatively, use <code class="literal">--cloud-id</code>.</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO1-3"><i class="conum" data-value="3"></i></a></p>
</td>
<td align="left" valign="top">
<p>Specify the identifier for the model in the Hugging Face model hub.</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO1-4"><i class="conum" data-value="4"></i></a></p>
</td>
<td align="left" valign="top">
<p>Specify the type of NLP task. Supported values are <code class="literal">fill_mask</code>, <code class="literal">ner</code>,
<code class="literal">question_answering</code>, text_classification`, <code class="literal">text_embedding</code>, and <code class="literal">zero_shot_classification</code>.</p>
</td>
</tr>
</table>
</div>
<div class="pre_wrapper lang-python">
<pre class="programlisting prettyprint lang-python">&gt;&gt;&gt; import elasticsearch
&gt;&gt;&gt; from pathlib import Path
&gt;&gt;&gt; from eland.ml.pytorch import PyTorchModel
&gt;&gt;&gt; from eland.ml.pytorch.transformers import TransformerModel

# Load a Hugging Face transformers model directly from the model hub
&gt;&gt;&gt; tm = TransformerModel("elastic/distilbert-base-cased-finetuned-conll03-english", "ner")
Downloading: 100%|██████████| 257/257 [00:00&lt;00:00, 108kB/s]
Downloading: 100%|██████████| 954/954 [00:00&lt;00:00, 372kB/s]
Downloading: 100%|██████████| 208k/208k [00:00&lt;00:00, 668kB/s]
Downloading: 100%|██████████| 112/112 [00:00&lt;00:00, 43.9kB/s]
Downloading: 100%|██████████| 249M/249M [00:23&lt;00:00, 11.2MB/s]

# Export the model in a TorchScript representation which Elasticsearch uses
&gt;&gt;&gt; tmp_path = "models"
&gt;&gt;&gt; Path(tmp_path).mkdir(parents=True, exist_ok=True)
&gt;&gt;&gt; model_path, config, vocab_path = tm.save(tmp_path)

# Import model into Elasticsearch
&gt;&gt;&gt; es = elasticsearch.Elasticsearch("http://elastic:mlqa_admin@localhost:9200", timeout=300)  # 5 minute timeout
&gt;&gt;&gt; ptm = PyTorchModel(es, tm.elasticsearch_model_id())
&gt;&gt;&gt; ptm.import_model(model_path=model_path, config_path=None, vocab_path=vocab_path, config=config)
100%|██████████| 63/63 [00:12&lt;00:00,  5.02it/s]</pre>
</div>
<h4><a id="ml-nlp-pytorch-auth"></a>Authentication methods<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/eland/edit/main/docs/guide/machine-learning.asciidoc">edit</a></h4>
<p>The following authentication options are available when using the import script:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<p>username and password authentication (specified with the <code class="literal">-u</code> and <code class="literal">-p</code> options):</p>
<div class="pre_wrapper lang-bash">
<pre class="programlisting prettyprint lang-bash">eland_import_hub_model -u &lt;username&gt; -p &lt;password&gt; --cloud-id &lt;cloud-id&gt; ...</pre>
</div>
<p>These <code class="literal">-u</code> and <code class="literal">-p</code> options also work when you use <code class="literal">--url</code>.</p>
</li>
<li class="listitem">
<p>username and password authentication (embedded in the URL):</p>
<div class="pre_wrapper lang-bash">
<pre class="programlisting prettyprint lang-bash">eland_import_hub_model --url https://&lt;user&gt;:&lt;password&gt;@&lt;hostname&gt;:&lt;port&gt; ...</pre>
</div>
</li>
<li class="listitem">
<p>API key authentication:</p>
<div class="pre_wrapper lang-bash">
<pre class="programlisting prettyprint lang-bash">eland_import_hub_model --es-api-key &lt;api-key&gt; --url https://&lt;hostname&gt;:&lt;port&gt; ...</pre>
</div>
</li>
</ul>
</div>
</div>
<div class="navfooter">
<span class="prev">
<a href="dataframes.html">« Data Frames</a>
</span>
<span class="next">
</span>
</div>
</div>
</body>
</html>
